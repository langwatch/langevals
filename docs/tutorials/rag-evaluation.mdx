---
title: "RAG Evaluation"
---

In this tutorial we will show how you can evaluate your RAG application with the help of LangEvals.
Check [RAGs Context Tracking](/integration/rags-context-tracking) to learn how to integrate your application with LangWatch and trace your RAG calls.


## LangEvals RAG Evaluators
You can easily import RAG evaluators from the `langevals_ragas` module and quickly make use of their features.
```python
from langevals_ragas.context_relevancy import RagasContextRelevancyEntry, RagasContextRelevancyEvaluator
from langevals_ragas.faithfulness import RagasFaithfulnessEntry, RagasFaithfulnessEvaluator
from langevals_ragas.answer_relevancy import RagasAnswerRelevancyEntry, RagasAnswerRelevancyEvaluator

entry1 = RagasAnswerRelevancyEntry(input="What is the capital of France?", output="Paris is the capital of France")

entry2 = RagasContextRelevancyEntry(output="Paris is the capital of France", contexts=[
    "Water can evaporate or turn into ice",
    "Dogs and Cats can be friends",
    "The sun is shining today"
])

entry3 = RagasFaithfulnessEntry(output="Paris is the capital of France", contexts=[
    "France is a country in Europe",
    "Lyon, Paris and Bordeaux are cities in France",
    "Paris is the capital of France"
])

result1 = RagasAnswerRelevancyEvaluator().evaluate(entry=entry1)
result2 = RagasContextRelevancyEvaluator().evaluate(entry=entry2)
result3 = RagasFaithfulnessEvaluator().evaluate(entry=entry3)
```
In the example above we import 3 evaluators and create 3 corresponding entries that are evaluated at the end.
Each entry requires different set of input parameters, depending on what we want to evaluate. Pay attention to the `entry2`,
how do you think, what will be the result of its evaluation on context relevancy?
<AccordionGroup>
<Accordion title="Example RAG Evaluations Output">
As you can see the result of the evaluation is an object with a few attributes. It returns the status of the evaluation,
the score of the evaluation and the cost (as far as RAG evaluators use another LLM to perform evaluation).
```bash
status='processed' score=1.0 passed=None details=None cost=Money(currency='USD', amount=0.002036)
status='processed' score=0.3333333333333333 passed=None details=None cost=Money(currency='USD', amount=0.00033600000000000004)
status='processed' score=1.0 passed=None details=None cost=Money(currency='USD', amount=0.0038910000000000004)
```
The second result corresponds to the `entry2` being evaluated on context relevancy. With no surprise we observe a low score of 0.333,
as the output is absolutely unrelated to the given contexts. This might be a sign that your RAG is retrieving the wrong documents or that
it generates responses that are not based on the given contexts.
</Accordion>
</AccordionGroup>

<Card title="Open in Notebook" icon="github" href="https://github.com/langwatch/langevals/blob/main/notebooks/tutorials/rag_evaluation.ipynb">
    You can access and run the code yourself in Jupyter Notebook
</Card>